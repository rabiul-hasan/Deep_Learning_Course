{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 (100 points)\n",
    "\n",
    "You are expected to complete this notebook with lines of code, plots, texts, and/or equations. In this assignment, you will encounter three kinds of questions:\n",
    "- Coding questions (mark with red letter <font color='red'>**C**</font>) - this type of question requires you to provide an implementation of a certain task.\n",
    "- Theoretical questions (mark with magneta letter <font color='magneta'>**T**</font>) - this type of question requires you to provide an answer with text or mathematical equations. When an equation is asked, you can write it in python/numpy syntax (using \\`\\`) or in latex syntax (using \\$\\$).\n",
    "- Analysis questions (mark with blue letter <font color='blue'>**A**</font>) - this type of question requires an analysis of results that may include text, code and visualizations. The first questions of this type will be more explicit of what you should write. They will become more open-ended as the assignment goes on.\n",
    "\n",
    "For assignment submission, you will submit this notebook file (.ipynb) through Canvas with cells executed and outputs visible. Your submitted notebook needs to follow these guidelines:\n",
    "\n",
    "- No other packages than the ones already imported can be used.\n",
    "- No other data than the ones provided should be used.\n",
    "- The cell outputs present in your delivered notebook should be reproducible.\n",
    "- Existing cells that require your input with **code** will be marked with comments `##your code starts here` and `##your code ends here` to specify where you need to write code.\n",
    "- Your final delivery should **NOT** contain any additional modifications outside of the demarcations defined above. However, if needed for testing, please feel free to modify it and remember to reverse to its original state before you submit it.\n",
    "- All code must be your own work. Code cannot be copied from external sources or another students. You may copy code from cells that were pre-defined in this notebook if you think it is useful for use in another question.\n",
    "- All images must be generated from data generated in your code. Do **NOT** import/display images that are generated outside your code.\n",
    "- Your analysis must be your own, but if you quote text or equations from another source please make sure to cite the appropriate references.\n",
    "\n",
    "Other notes:\n",
    "- Cells should be run in order, using Shift+Enter.\n",
    "- Read all the provided code cells and comments, as they contain variables and information that you may need to use to complete the notebook.\n",
    "- Ends of questions are marked with **END**. \n",
    "- To create a text cell, create it with the \"+\" button and change its type from \"Code\" to \"Markdown\" using the upper menu. \n",
    "- To modify a text cell, double click on it.\n",
    "- If you are interested, you can check detail on formatting markdown text here: https://medium.com/ibm-data-science-experience/markdown-for-jupyter-notebooks-cheatsheet-386c05aeebed\n",
    "- A python/numpy/matplotlib tutorial that you might find useful: http://cs231n.github.io/python-numpy-tutorial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex. 0 - Setting up libraries and useful functions **(Total of 0 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#install a few libraries that will be used\n",
    "!pip3 install matplotlib\n",
    "!pip3 install numpy\n",
    "!pip3 install pandas\n",
    "!pip3 install python-mnist\n",
    "\n",
    "#import a few libraries. Numpy is named as np and pyplot in matplotlib as plt\n",
    "import urllib.request\n",
    "import pandas\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from utils import *\n",
    "\n",
    "#needed to plot plots with matplotlib in OSX\n",
    "%matplotlib inline\n",
    "\n",
    "#set numpy to raise exceptions when encountering numerical errors\n",
    "np.seterr(all='raise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is used to convert from integer encoding of labels to one hot encoding\n",
    "# labels is an 1-D array with the integer labels from 0 to n_labels. \n",
    "def one_hot(labels, n_labels):\n",
    "    return np.squeeze(np.eye(n_labels)[labels.reshape(-1)])\n",
    "\n",
    "#Does the transpose of the last two axes of a tensor\n",
    "def T(input_tensor):\n",
    "    return np.swapaxes(input_tensor, -1, -2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1 - Analyzing model capacity with a polynomial toy example **(Total of 30 points)**\n",
    "\n",
    "This exercise will illustrate how validation error of a model evolves by changing model capacity. We are going to start with a simple example that follows a third degree polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1.1(C)</font> (2 points)\n",
    "\n",
    "In this section, you will implement a function, named `third_degree_polynomial`, that returns the output of a third degree polynomial. This function receives two numpy arrays, `x` and `coeffs`. If `coeffs` $= [a_0, a_1, a_2, a_3]$, the function should return an output array `y` in which $y_i = a_3\\times x_i^3 + a_2\\times x_i^2 + a_1\\times x_i + a_0$ for $i$ sample. Note: the output array, `y`, should have shape of [N, 1] where N is the number of samples in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def third_degree_polynomial(x, coeffs):\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "validate_ex11(third_degree_polynomial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.1(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to set up data for Exercise 1.2-1.6 and 2. Please pay close attention to the names of the variables because you are going to need to use them in your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Generating input data and target data for this exercise\n",
    "#noise is added to data to make the fitting inexact\n",
    "\n",
    "#coefficients of the third degree polynomial used to generate the data\n",
    "coeffs_ex1 = np.array([[3.2,-1,5,-2.2]]).T\n",
    "\n",
    "#Training data input (you are going to use for exercises 1 and 2)\n",
    "x_ex1_train = np.expand_dims(np.arange(0,2,0.1), axis = 1)\n",
    "x_ex1_val = np.expand_dims(np.arange(0,2,0.1), axis = 1)\n",
    "\n",
    "\n",
    "#Target data\n",
    "y_ex1_train = third_degree_polynomial(x_ex1_train, coeffs_ex1)\n",
    "y_ex1_val = third_degree_polynomial(x_ex1_val,coeffs_ex1) \n",
    "\n",
    "#add noise to target data\n",
    "np.random.seed(1)\n",
    "epsilon_ex1 = 0.35\n",
    "y_ex1_train = y_ex1_train + epsilon_ex1 * np.random.normal(size = x_ex1_train.shape)\n",
    "y_ex1_val = y_ex1_val + epsilon_ex1 * np.random.normal(size = x_ex1_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1.2(C)</font> (5 points)\n",
    "To fit polynomial functions to the data that we just generated, your task is to implement a polynomial least square fitting function, named `fit_func`, that receives 3 inputs (input array, target array, and the degree of the polynomial fitting function) and returns the coeffficients array (of shape [degree+1, 1]) of the polynomial fitting function. Moreover, a polynomial fitting function can be expressed in a closed-form solution. Hence, your `fit_func` needs to be implemented with the closed-form solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_func(inputs, targets, degree):\n",
    "    ##your code starts here\n",
    "   \n",
    "\n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.2(C)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "validate_ex12(fit_func, x_ex1_train, y_ex1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1.3(C)</font> (4 points)\n",
    "In exercise 1.1 above, we implement a third degree polynominal function. Now, we will generalize it to express the polynomial function of any degrees. In this section, you need to implement a function, named `any_degree_polynomial`, that receives two numpy arrays, `x` and `coeffs`, and computes the polynomial function value for each sample in `x`. For example, if `coeffs` $= [a_0,a_1,...,a_n]$, the function should return the value of $i$ sample as $y_i = a_n\\times x_i^n + a_{n-1}\\times x_i^{n-1} + ... + a_1\\times x_i + a_0$. Note: the output array, `y`, should have shape of [N, 1] where N is the number of samples in `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def any_degree_polynomial(x, constants_vector):\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.3(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1.4(C)</font> (4 points)\n",
    "Now, you will use the functions above to fit a third degree polynomial to the provided data `(x_ex1_train,y_ex1_train)`, and plot your fitted results and the training data on the same graph. Please remember to use legend to identify what each curve is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Fit a 3rd degree polynomial to data and visualize the result of your fit\n",
    "\n",
    "##your code starts here\n",
    "\n",
    "\n",
    "##your code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.4(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next two exercises, we are going to evaluate how polynomials of different degrees perform in the validation data when fitted to the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 1.5(C) </font> (2 points)\n",
    "First, we are going to define a function that returns the metric used to evaluate the results. For this exercise, we use the mean squared error function, which is defined as $\\frac{1}{N}\\sum{\\left( \\hat{y} - y \\right)^{2}}$. Your task is to implement a function, named `mse`, that takes two numpy arrays (`predicted_values` and `targets`) as input and returns the mean squared error between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predicted_values, targets):\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "validate_ex15(mse, y_ex1_val[:20,:],y_ex1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.5(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magneta'>Exercise 1.6(T)</font> (3 points)\n",
    "Maximum likelihood (ML) principle is a common way to derive a good estimator for different models. For example, we can use ML to derive the estimation of the coefficients of polynomial fitting functions. If using ML, what assumptions do we need to make in order to obtain the same estimated coefficients as using the mean squared error criterion in Exercise 1.5 above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here:**\n",
    "(Double click on this cell to enter text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.6(T)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Exercise 1.7(A)</font> (10 points)\n",
    "Now, we are going to treat the degree of the fitting polynomial as a hyperparameter. In this section, you will:\n",
    "1) Fit the training data, `x_ex1_train`, and validation data, `x_ex1_val`, using polynomials of degree 1 to 10. Then, plot the mean squared error as a function of polynomial degrees for both training and validation data on the same graph.\n",
    "\n",
    "2) Write a short analysis of the results presented in your plot, stating which degrees are overfitting and which are underfitting, and why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##your code starts here\n",
    "\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your analysis of the results here:** (Double click on this cell to enter text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 1.7(A)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Defining and training fully connected networks **(Total of 25 points)**\n",
    "\n",
    "In this exercise, we are going to define functions to train a fully connected network with one hidden layer. First, we set a function to initialize the learnable parameters of the network. They are going to be arrays stored in a python dictionary in which the keys of the dictionary represent the name of the parameters of the network. The parameters are called 'weights_i' and 'bias_i' where i is the layer where the parameter is used. For the linear layer forward pass, we use the equation/notation $XW+b$, $X$ being a matrix with batch size as first dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_ex2(n_inputs, n_hidden_nodes, n_outputs):\n",
    "    np.random.seed(1)\n",
    "    #initialize weights centered in 0\n",
    "    weights_1 = np.random.normal(0,0.5,[n_inputs,n_hidden_nodes])\n",
    "    # initialize bias with a small positive value to reduce amount of dead neurons\n",
    "    bias_1 = np.random.normal(0.1,0,[n_hidden_nodes])\n",
    "    #initialize weights centered in 0\n",
    "    weights_2 = np.random.normal(0,0.5,[n_hidden_nodes,n_outputs])\n",
    "    # initialize bias with a small positive value to reduce amount of dead neurons\n",
    "    bias_2 = np.random.normal(0.1,0,[n_outputs])\n",
    "    return {'weights_1':weights_1, 'weights_2':weights_2, 'bias_1':bias_1, 'bias_2':bias_2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magneta'>Exercise 2.1(T)</font> (1 points) \n",
    "Calculate the number of parameters of a network initialized with initialize_parameters_ex2 function in Exercise 2.1. Your answer needs to be expressed as a function of n_inputs, n_hidden_nodes and n_outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answer here:** (Double click on this cell to enter text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 2.1(T)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 2.2(C)</font> (4 points)\n",
    "Complete the function, `two_layer_network_forward`, below to define a forward pass of a two-layer fully connected network with ReLU as the activation function of the first layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_layer_network_forward(inputs, parameters, return_intermediary_results = False):\n",
    "    \n",
    "    ##your code starts here    \n",
    "    \n",
    "    \n",
    "    ##your code ends here\n",
    "    \n",
    "    #return_intermediary_results should only be True if you are going to use this forward pass\n",
    "    # to calculate gradients for the network parameters\n",
    "    if return_intermediary_results:\n",
    "        #if you are doing the forward pass to calculate backward pass afterwards, you are going to need all intermediary results\n",
    "        to_return = {'out_1': out_1, 'out_1_relu': out_1_relu, 'out_2': out_2}\n",
    "    else:\n",
    "        #if you are doing the forward pass just to get the output of the network, you only need the final result\n",
    "        to_return = out_2\n",
    "    return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "validate_ex22(two_layer_network_forward, initialize_parameters_ex2, x_ex1_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 2.2(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='magneta'>Exercise 2.3(T)</font> (7 points) \n",
    "In this section, we will derive the derivaties of the parameters (in matrix form) of a feed-forward network using backpropagation algorithm. Given the two-layer network specified in Exercise 2.2 above, the network can be defined with the following operations where $\\mathbf{x}, \\mathbf{\\hat{y}}$ are the input and output of the network, respectively:\n",
    "\n",
    "$\\mathbf{z} = \\mathbf{x}\\mathbf{w}_1 + \\mathbf{b}_1$\n",
    "\n",
    "$\\mathbf{h} = relu(\\mathbf{z})$\n",
    "\n",
    "$\\mathbf{\\hat{y}} = \\mathbf{h}\\mathbf{w}_2 + \\mathbf{b}_2$\n",
    "\n",
    "Please derive the derivatives of the parameters in the network if the mean squared error is used as the cost function, i.e., $\\frac{\\partial L}{\\partial\\mathbf{W_1}}$, $\\frac{\\partial L}{\\partial\\mathbf{b_1}}$, $\\frac{\\partial L}{\\partial\\mathbf{W_2}}$, and $\\frac{\\partial L}{\\partial\\mathbf{b_2}}$. Note: You need to show a step-by-step of how you obtain each derivatives to receive full credit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your answers here:** (Double click on this cell to enter text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 2.3(T)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 2.4(C)</font> (8 points) \n",
    "Now, you will implement two functions `mse_loss_backward` and `two_layer_network_backward` for the derivatives of the cost function and parameters of the network that you derive in Exercise 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss_backward(predicted, gt):\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here\n",
    "    return derivative_of_mse_loss_with_respect_to_predicted\n",
    "\n",
    "def two_layer_network_backward(inputs, parameters, gt, loss_backward):\n",
    "    \n",
    "    intermediary_results_in_forward = two_layer_network_forward(inputs, parameters, return_intermediary_results = True)\n",
    "\n",
    "    out_1 = intermediary_results_in_forward['out_1'] \n",
    "    out_1_relu = intermediary_results_in_forward['out_1_relu'] \n",
    "    out_2 = intermediary_results_in_forward['out_2'] \n",
    "    \n",
    "    derivative_of_loss_with_respect_to_out_2 = loss_backward(out_2, gt) \n",
    "    \n",
    "    ##your code starts here\n",
    "        \n",
    "    \n",
    "    ##your code ends here\n",
    "    \n",
    "    return {\n",
    "            'weights_1': derivative_of_loss_with_respect_to_weights_1,\n",
    "            'bias_1': derivative_of_loss_with_respect_to_bias_1, \n",
    "            'weights_2':derivative_of_loss_with_respect_to_weights_2 , \n",
    "            'bias_2':derivative_of_loss_with_respect_to_bias_2\n",
    "            }\n",
    "\n",
    "def two_layer_network_mse_backward(inputs, parameters, gt):\n",
    "    return two_layer_network_backward(inputs, parameters, gt, mse_loss_backward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "test_gradient(two_layer_network_forward, two_layer_network_mse_backward, mse, x_ex1_train[0:20,:], y_ex1_train[0:20,:], initialize_parameters_ex2(1, 10, 1) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 2.4(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 2.5(C)</font>  (5 points)\n",
    "In this section, you are going to implement the update rule for a batch of training. Complete the function,`run_batch_sgd`, below that calculates the gradients and then updates the parameters using the vanilla stochastic gradient descent update rule. Then, you will use this function for the training process defined in the next cell and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_batch_sgd(backward_function, parameters, learning_rate, inputs, targets):\n",
    "    #calculate gradients and update parameters using sgd update rule\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here\n",
    "    return updated_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_hidden_nodes = 50\n",
    "parameters_two_layer_regression = initialize_parameters_ex2(1, n_hidden_nodes, 1)\n",
    "\n",
    "learning_rate = 0.001\n",
    "batch_size = 20\n",
    "n_epochs = 1000\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    shuffled_indexes = (np.arange(x_ex1_train.shape[0]))\n",
    "    np.random.shuffle(shuffled_indexes)\n",
    "    shuffled_indexes = shuffled_indexes.reshape([-1, batch_size])\n",
    "    for batch_i in range(shuffled_indexes.shape[1]):    \n",
    "        batch = shuffled_indexes[:,batch_i]\n",
    "        input_this_batch = x_ex1_train[batch,:]\n",
    "        gt_this_batch =  y_ex1_train[batch,:]\n",
    "        #use you function run_batch_sgd to update the parameters\n",
    "        ##your code starts here\n",
    "        \n",
    "        \n",
    "        ##your code ends here\n",
    "\n",
    "#plot the results of training\n",
    "##your code starts here\n",
    "\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 2.5(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - Classification task using a medical dataset with cross-entropy and softmax (Total of 22 points)\n",
    "\n",
    "We are going to apply what we just defined in Exercise 2 to a real world application. The problem, that we want to solve here, is to predict if a patient had a seizure from a electroencephalogram (EEG) signal. We will use a dataset (https://archive.ics.uci.edu/ml/datasets/Epileptic+Seizure+Recognition) that contains preprocessed 1-D EEG signals and corresponding labels, i.e., if the patients had seizure or not. Your task will be to implement a feed-forward network using functions defined in Exercise 2 to solve this problem and analyze the effect of hyperparameters having on the network's performance. Before we can begin the implementation of the network, we need to load and preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the csv file. Note: Please make sure the data_ex3.csv file is in the same directory\n",
    "data_ex3 = pandas.read_csv('./data_ex3.csv')\n",
    "\n",
    "#modify labels since we are going to do a binary classification, seizure or no seizure, \n",
    "# and presence of seizure is represented by label 1\n",
    "data_ex3.y = data_ex3.y.map({1:1,2:0,3:0,4:0,5:0})\n",
    "\n",
    "#visualizing the current table of the loaded dataset to see how it is organized\n",
    "data_ex3[:5]\n",
    "\n",
    "# preprocess the data you just loaded\n",
    "train_data_ex3, val_data_ex3, test_data_ex3, train_labels_ex3, val_labels_ex3, test_labels_ex3 =  preprocess_medical_data(data_ex3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sanity check of the result of normalization, one-hot encoding and shapes of the data vectors\n",
    "print('train_data_ex3:\\n ' + str(train_data_ex3))\n",
    "print('train_labels_ex3:\\n ' + str(train_labels_ex3))\n",
    "print('train_data_ex3.shape: ' + str(train_data_ex3.shape))\n",
    "print('train_labels_ex3.shape: ' + str(train_labels_ex3.shape))\n",
    "print('val_data_ex3.shape: ' + str(val_data_ex3.shape))\n",
    "print('val_labels_ex3.shape: ' + str(val_labels_ex3.shape))\n",
    "print('test_data_ex3.shape: ' + str(test_data_ex3.shape))\n",
    "print('test_labels_ex3.shape: ' + str(test_labels_ex3.shape))\n",
    "\n",
    "#checking how many of the labels are seizure labels\n",
    "#the dataset is unbalanced, but you should use it just like that\n",
    "print('Percentage of examples containing seizures: ' + str(np.sum(train_labels_ex3[:,1])/float(len(train_labels_ex3))*100) + '%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we are going to define a few more functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a softmax calculation with numerical stability tricks\n",
    "def softmax(logits, axis):\n",
    "    # subtracting the maximum logit from all logits for each example and prevents overflow \n",
    "    # of the exponential function of the logits and does not change results of the softmax\n",
    "    # because of properties of division of exponentials\n",
    "    stabilizing_logits = logits - np.expand_dims(np.max(logits, axis = axis), axis = axis)\n",
    "    \n",
    "    # clipping all logits to a minimum of -10 prevents underflow of the exponentials and \n",
    "    # only changes the result of the softmax minimally, since we know that one logit has value 0\n",
    "    # and exp^0>>exp(-10)\n",
    "    stabilizing_logits = np.clip(stabilizing_logits, -10, None)\n",
    "    \n",
    "    #using the softmax classic equation, but with the modified logits to prevent numerical errors\n",
    "    return np.exp(stabilizing_logits)/np.expand_dims(np.sum(np.exp(stabilizing_logits), axis = axis), axis = axis)\n",
    "\n",
    "# a forward function combined the two-layer network and the softmax\n",
    "def two_layer_network_softmax_forward(inputs, parameters):\n",
    "    logits = two_layer_network_forward(inputs, parameters)\n",
    "    return softmax(logits, axis = 1)\n",
    "\n",
    "# a forward function combined the two-layer network and the softmax\n",
    "def softmax_plus_ce_loss_backward(predicted, gt):\n",
    "    #the derivative of the output of softmax function followed by a cross-entropy loss\n",
    "    # with respect to the input is a beautifully simple equation equals to the softmax\n",
    "    # of the inputs minus the one-hot encoded groundtruth\n",
    "    return (softmax(predicted, axis = 1) - gt)/predicted.shape[0]\n",
    "\n",
    "#the calculation of the gradient for the classification network\n",
    "def two_layer_network_softmax_ce_backward(inputs, parameters, gt):\n",
    "    return two_layer_network_backward(inputs, parameters, gt, softmax_plus_ce_loss_backward)\n",
    "\n",
    "# a function to get how many logits predicted the right class when compared to gt\n",
    "def count_correct_predictions(logits, gt):\n",
    "    predicted_labels = one_hot(np.argmax(logits, axis = 1), logits.shape[1])\n",
    "    return np.sum(np.logical_and(predicted_labels,gt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 3.1(C)</font> (3 points)\n",
    "Before putting together the training process, we need to define the loss function. Here, we are going to implement a cross-entropy loss between the predicted output and one-hot encoded target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(predicted_output, target):\n",
    "    ##your code starts here\n",
    "    \n",
    "    \n",
    "    ##your code ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test your function to make sure it is doing what is expected\n",
    "validate_ex31(ce_loss,softmax, two_layer_network_softmax_forward, two_layer_network_softmax_ce_backward,\\\n",
    "             train_data_ex3[0:10,:], train_labels_ex3[0:10,:], initialize_parameters_ex2(178, 10, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 3.1(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Exercise 3.2(A)</font> (16 points)\n",
    "In this section, you will first put together the training process of a two-layer neural network. You should be able to get a validation accuracy higher than 96% with the provided learning rate, batch size, and number of epochs for this dataset. Then, you will analyze how the accuracy changes as a function of the number of hidden nodes.\n",
    "\n",
    "Hints:\n",
    "- Question 2.4 has codes that may be useful here. \n",
    "- In addition to the codes from question 2.4, you will need to loop over different numbers of hidden nodes to train different models.\n",
    "- Write the inference loops to get the training and the validation accuracy of your model. \n",
    "- For the accuracy calculation, you may find the provided `count_correct_predictions` function useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01 \n",
    "batch_size = 50\n",
    "n_epochs = 100\n",
    "\n",
    "##your code starts here\n",
    "\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your analysis here:** (Double click on this cell to enter text) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 3.2(A)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 3.3(C)</font> (3 points)\n",
    "Lastly, we will run the best model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indexes = (np.arange(test_data_ex3.shape[0]))\n",
    "shuffled_indexes = np.array_split(shuffled_indexes,test_data_ex3.shape[0]//batch_size )\n",
    "corrects = 0\n",
    "total = 0\n",
    "n_hidden_nodes = 100\n",
    "for batch_i in range(len(shuffled_indexes)):\n",
    "    batch = shuffled_indexes[batch_i]\n",
    "    corrects += count_correct_predictions(two_layer_network_forward(test_data_ex3[batch], parameters_two_layer_classification_dic[n_hidden_nodes]), test_labels_ex3[batch])\n",
    "    total += len(batch)\n",
    "print('Test accuracy = ' + str(corrects/float(total)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 3.3(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4 - MNIST and weight decay **(Total of 23 points)**\n",
    "\n",
    "In this exercise, we are going to extend the two-layer neural network to another dataset. We are going to use a flattened and reduced MNIST dataset to train the network using L2 regularization. First, we need to load and preprocess the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_ex4_train, x_ex4_val, x_ex4_test, y_ex4_train, y_ex4_val, y_ex4_test = load_and_preprocess_mnist()\n",
    "\n",
    "#sanity check to see that data is as it is supposed to be\n",
    "plt.imshow(x_ex4_train[1000,:].reshape(28,28), cmap = 'Greys')\n",
    "print(y_ex4_train[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 4.1(C)</font> (4 points)\n",
    "The $L_2$ penalty is defined as the sum of squares of every element of the penalized parameters. In this section, you will implement the gradients of different parameters in the network. We consider the $L_2$ penalty over all the weights parameters, and no penalty over the bias parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization_backward(inputs, parameters, gt):\n",
    "    gradients = {}\n",
    "    for parameter_name in parameters.keys():\n",
    "        if 'weights' in parameter_name:\n",
    "            # complete the equation to calculate the l2 regularization loss gradient for weights\n",
    "            ##your code starts here\n",
    "            \n",
    "            \n",
    "            ##your code ends here\n",
    "        elif 'bias' in parameter_name:\n",
    "            # complete the equation to calculate the l2 regularization loss gradient for bias.\n",
    "            # Remember, the L2 regularization loss for bias is 0.\n",
    "            ##your code starts here\n",
    "            \n",
    "            \n",
    "            ##your code ends here\n",
    "    return gradients\n",
    "\n",
    "def two_layer_network_ce_and_l2_regularization_backward(inputs, parameters, gt, regularization_multiplier):\n",
    "    gradients = {}\n",
    "    gradients1 = two_layer_network_softmax_ce_backward(inputs, parameters, gt)\n",
    "    gradients2 = l2_regularization_backward(inputs, parameters, gt)\n",
    "    for parameter_name in parameters:\n",
    "        gradients[parameter_name] = gradients1[parameter_name] + regularization_multiplier * gradients2[parameter_name]\n",
    "    return gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 4.1(C)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Exercise 4.2(A)</font> (16 points)\n",
    "In this section, you will analyze how the weight decay, $\\lambda$, of the $L_2$ regularization changes the final results for the MNIST dataset using a two-layer neural network with 200 neurons in the hidden layer. \n",
    "\n",
    "Notes:\n",
    "- You should play a bit with the learning rate, batch size and number of epochs.\n",
    "- You should be able to get more than 90% accuracy on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_hidden_nodes = 200\n",
    "\n",
    "##your code starts here\n",
    "\n",
    "\n",
    "##your code ends here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write your analysis here:** (Double click on this cell to enter text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 4.2(A)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='red'>Exercise 4.3(C)</font> (3 points)\n",
    "Finally, test your best model using the provided test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indexes = (np.arange(x_ex4_test.shape[0]))\n",
    "shuffled_indexes = np.array_split(shuffled_indexes,x_ex4_test.shape[0]//batch_size )\n",
    "corrects = 0\n",
    "total = 0\n",
    "for batch_i in range(len(shuffled_indexes)):\n",
    "    batch = shuffled_indexes[batch_i]\n",
    "    corrects += count_correct_predictions(two_layer_network_forward(x_ex4_test[batch], parameters_two_layer_classification_ex4_dic[0.001]), y_ex4_test[batch])\n",
    "    total += len(batch)\n",
    "print('Test accuracy = ' + str(corrects/float(total)*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**END 4.3(C)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
